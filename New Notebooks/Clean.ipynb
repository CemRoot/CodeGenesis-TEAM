{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-19T23:25:19.162397Z",
     "start_time": "2024-12-19T23:25:16.426439Z"
    }
   },
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "In this notebook, we are taking the raw CSV files and performing a complete data cleaning process.\n",
    "We will:\n",
    "- Verify that the raw files exist before we attempt to load them\n",
    "- Load the raw data into Pandas DataFrames\n",
    "- Standardize column names to a consistent, snake_case format so we can handle them uniformly\n",
    "- Parse and unify date columns into a proper datetime format\n",
    "- Handle missing values by either filling them with a suitable default (like 0 or mean) or dropping rows that are too incomplete\n",
    "- Remove duplicate rows that might skew our analysis\n",
    "- Optimize numeric types to reduce memory usage and improve performance\n",
    "- Provide optional steps to detect and remove extreme outliers from numeric columns\n",
    "- Ensure categorical columns are recognized and possibly encoded or documented\n",
    "- Sort data by date where it makes sense, ensuring chronological order for time series\n",
    "- Log these steps into a cleaning.log file for traceability and debugging if needed\n",
    "\n",
    "This approach ensures our datasets are clean, consistent, and analysis-ready.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Setup logging so we have a trace of our cleaning steps\n",
    "# ----------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    filename='cleaning.log',      # We log all steps here\n",
    "    filemode='a',                 # 'a' means append to existing log, don't overwrite\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Include timestamp and severity\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "logging.info(\"=== Starting the data cleaning process ===\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Ensure our processed directory exists, so we can save cleaned outputs there\n",
    "# ----------------------------------------------------\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# File paths to our raw datasets\n",
    "# Update these if your paths or filenames differ\n",
    "# ----------------------------------------------------\n",
    "path_vacc_death_rate = '../data/raw/covid-vaccinations-vs-covid-death-rate.csv'\n",
    "path_vacc_manufacturer = '../data/raw/covid-vaccine-doses-by-manufacturer.csv'\n",
    "path_oecd = '../data/raw/OECD_health_expenditure.csv'\n",
    "path_us_death_rates = '../data/raw/united-states-rates-of-covid-19-deaths-by-vaccination-status.csv'\n",
    "\n",
    "\n",
    "def check_file_exists(filepath):\n",
    "    \"\"\"\n",
    "    Before we load a file, we want to ensure it actually exists.\n",
    "    If it doesn't, we log the error and raise an exception.\n",
    "    This way we don't try to clean data we don't have.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        logging.error(f\"File not found: {filepath}\")\n",
    "        raise FileNotFoundError(f\"Required file not found: {filepath}\")\n",
    "    else:\n",
    "        logging.info(f\"File found: {filepath}\")\n",
    "\n",
    "# Check that all our input files exist\n",
    "check_file_exists(path_vacc_death_rate)\n",
    "check_file_exists(path_vacc_manufacturer)\n",
    "check_file_exists(path_oecd)\n",
    "check_file_exists(path_us_death_rates)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Helper functions\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def standardize_column_names(df):\n",
    "    \"\"\"\n",
    "    Convert column names to snake_case and remove problematic characters.\n",
    "    Explicitly set regex=False to avoid any unintended regex interpretation.\n",
    "    \"\"\"\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(' ', '_', regex=False)\n",
    "        .str.replace('(', '', regex=False)\n",
    "        .str.replace(')', '', regex=False)\n",
    "        .str.replace('-', '_', regex=False)\n",
    "        .str.replace('/', '_', regex=False)\n",
    "        .str.replace('&', 'and', regex=False)\n",
    "        .str.replace('__', '_', regex=False)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def optimize_numeric_types(df):\n",
    "    \"\"\"\n",
    "    Downcast numeric columns to reduce the memory usage of our DataFrame.\n",
    "    This can be especially helpful if the dataset is large.\n",
    "    By converting floats to float32 and ints to int32 (or even int16),\n",
    "    we often reduce memory footprint without losing meaningful precision.\n",
    "    \"\"\"\n",
    "    for col in df.select_dtypes(include=[np.number]):\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    return df\n",
    "\n",
    "def log_dataframe_info(df, df_name):\n",
    "    \"\"\"\n",
    "    Log basic information about the DataFrame:\n",
    "    - Shape (rows and columns)\n",
    "    - Null counts per column\n",
    "    - Data types\n",
    "    - Memory usage\n",
    "\n",
    "    This helps us keep track of how our cleaning steps affect the dataset.\n",
    "    \"\"\"\n",
    "    logging.info(f\"--- {df_name} Info ---\")\n",
    "    logging.info(f\"Shape: {df.shape}\")\n",
    "    null_counts = df.isnull().sum()\n",
    "    logging.info(f\"Null counts:\\n{null_counts}\")\n",
    "    dtypes = df.dtypes\n",
    "    logging.info(f\"Data types:\\n{dtypes}\")\n",
    "    mem_usage = df.memory_usage(deep=True)\n",
    "    logging.info(f\"Memory Usage:\\n{mem_usage}\")\n",
    "    logging.info(\"-------------------------\")\n",
    "\n",
    "def parse_dates(df, date_columns):\n",
    "    \"\"\"\n",
    "    Convert specified columns into proper datetime objects.\n",
    "    If parsing fails for some rows, those become NaT (Not a Time).\n",
    "    Date parsing ensures we can sort chronologically and do time-based analysis.\n",
    "    \"\"\"\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    return df\n",
    "\n",
    "def fill_numeric_nas(df, fill_value=0):\n",
    "    \"\"\"\n",
    "    Fill missing values in numeric columns with a given fill_value (default 0).\n",
    "    This avoids errors during calculations and ensures we don't lose rows unnecessarily.\n",
    "    Adjust the strategy as needed:\n",
    "    - mean/median fill might be better in some cases\n",
    "    - forward/backfill if time series data\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(fill_value)\n",
    "    return df\n",
    "\n",
    "def remove_outliers(df, factor=3):\n",
    "    \"\"\"\n",
    "    OPTIONAL STEP:\n",
    "    Remove extreme outliers from numeric columns using the IQR method.\n",
    "    We take columns, compute Q1 and Q3, then define an acceptable range.\n",
    "    Anything beyond factor * IQR from Q1 or Q3 is considered an outlier.\n",
    "\n",
    "    This is highly dependent on the business case. Use with caution, as removing outliers\n",
    "    might remove valid but rare data points.\n",
    "\n",
    "    For demonstration, this function is defined but not automatically applied.\n",
    "    If you want to apply it, call remove_outliers(df) on your dataset.\n",
    "\n",
    "    NOTE: If certain columns shouldn't have outlier removal, exclude them.\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - (factor * IQR)\n",
    "        upper_bound = Q3 + (factor * IQR)\n",
    "        initial_count = len(df)\n",
    "        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "        removed = initial_count - len(df)\n",
    "        if removed > 0:\n",
    "            logging.info(f\"Removed {removed} outliers from '{col}' using factor={factor}\")\n",
    "    return df\n",
    "\n",
    "def handle_categorical_data(df):\n",
    "    \"\"\"\n",
    "    OPTIONAL/EXAMPLE STEP:\n",
    "    Identify categorical columns and possibly transform them if needed.\n",
    "    For now, we'll just log which columns appear categorical\n",
    "    and could be encoded (like using one-hot encoding) later if analysis requires.\n",
    "\n",
    "    If you know certain columns are categorical, you can cast them as 'category' dtype.\n",
    "    \"\"\"\n",
    "    object_cols = df.select_dtypes(include=['object']).columns\n",
    "    # Just an example: If a column has a low number of unique values, treat it as categorical\n",
    "    for col in object_cols:\n",
    "        unique_vals = df[col].nunique()\n",
    "        if unique_vals < 50:  # arbitrary threshold for categoricals\n",
    "            # Convert to categorical to reduce memory and clarify intention\n",
    "            df[col] = df[col].astype('category')\n",
    "            logging.info(f\"Converted '{col}' to categorical dtype with {unique_vals} categories.\")\n",
    "    return df\n",
    "\n",
    "def drop_empty_columns(df):\n",
    "    \"\"\"\n",
    "    Drop columns that are entirely empty (all NaNs).\n",
    "    This helps clean up unnecessary columns and keeps dataset tidy.\n",
    "    \"\"\"\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    return df\n",
    "\n",
    "def sort_by_date_if_available(df, date_column='day'):\n",
    "    \"\"\"\n",
    "    If the dataset has a 'day' or any date column,\n",
    "    sort the DataFrame by that column to ensure chronological order.\n",
    "    This is particularly helpful for time series data.\n",
    "    \"\"\"\n",
    "    if date_column in df.columns and pd.api.types.is_datetime64_any_dtype(df[date_column]):\n",
    "        df = df.sort_values(by=date_column)\n",
    "    return df\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Load the raw data\n",
    "# ----------------------------------------------------\n",
    "logging.info(\"Loading raw datasets...\")\n",
    "df_vdr = pd.read_csv(path_vacc_death_rate)\n",
    "df_vm = pd.read_csv(path_vacc_manufacturer)\n",
    "df_oecd = pd.read_csv(path_oecd)\n",
    "df_us = pd.read_csv(path_us_death_rates)\n",
    "\n",
    "# Log initial info about raw data\n",
    "log_dataframe_info(df_vdr, \"Vaccinations vs Death Rate (Raw)\")\n",
    "log_dataframe_info(df_vm, \"Vaccine Manufacturer (Raw)\")\n",
    "log_dataframe_info(df_oecd, \"OECD (Raw)\")\n",
    "log_dataframe_info(df_us, \"US Death Rates (Raw)\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Cleaning the Vaccinations vs COVID Death Rate dataset (df_vdr)\n",
    "# ----------------------------------------------------\n",
    "# Standardize columns so they're easier to reference\n",
    "df_vdr = standardize_column_names(df_vdr)\n",
    "\n",
    "# Parse date columns (assuming 'day' is the main date column)\n",
    "df_vdr = parse_dates(df_vdr, ['day'])\n",
    "\n",
    "# Remove duplicate rows that don't add value\n",
    "df_vdr.drop_duplicates(inplace=True)\n",
    "\n",
    "# Fill numeric missing values with 0\n",
    "df_vdr = fill_numeric_nas(df_vdr, fill_value=0)\n",
    "\n",
    "# If 'entity' is crucial, remove rows missing it\n",
    "if 'entity' in df_vdr.columns:\n",
    "    df_vdr = df_vdr[df_vdr['entity'].notna()]\n",
    "\n",
    "# Drop columns that are entirely empty\n",
    "df_vdr = drop_empty_columns(df_vdr)\n",
    "\n",
    "# Optimize numeric columns for memory\n",
    "df_vdr = optimize_numeric_types(df_vdr)\n",
    "\n",
    "# Attempt to handle categorical columns, if any\n",
    "df_vdr = handle_categorical_data(df_vdr)\n",
    "\n",
    "# Sort by date if it's meaningful (it likely is, since it's a timeseries)\n",
    "df_vdr = sort_by_date_if_available(df_vdr, 'day')\n",
    "\n",
    "# Log info after cleaning\n",
    "log_dataframe_info(df_vdr, \"Vaccinations vs Death Rate (Cleaned)\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Cleaning the Vaccine Manufacturer dataset (df_vm)\n",
    "# ----------------------------------------------------\n",
    "df_vm = standardize_column_names(df_vm)\n",
    "df_vm = parse_dates(df_vm, ['day'])\n",
    "df_vm.drop_duplicates(inplace=True)\n",
    "df_vm = fill_numeric_nas(df_vm, fill_value=0)\n",
    "\n",
    "if 'entity' in df_vm.columns:\n",
    "    df_vm = df_vm[df_vm['entity'].notna()]\n",
    "\n",
    "df_vm = drop_empty_columns(df_vm)\n",
    "df_vm = optimize_numeric_types(df_vm)\n",
    "df_vm = handle_categorical_data(df_vm)\n",
    "df_vm = sort_by_date_if_available(df_vm, 'day')\n",
    "log_dataframe_info(df_vm, \"Vaccine Manufacturer (Cleaned)\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Cleaning the OECD Health Expenditure dataset (df_oecd)\n",
    "# ----------------------------------------------------\n",
    "logging.info(\"Cleaning OECD_health_expenditure dataset...\")\n",
    "\n",
    "# Standardize column names\n",
    "df_oecd = standardize_column_names(df_oecd)\n",
    "\n",
    "# Remove duplicate columns if any\n",
    "df_oecd = df_oecd.loc[:, ~df_oecd.columns.duplicated()]\n",
    "\n",
    "# Convert time_period to numeric (if it represents a year)\n",
    "if 'time_period' in df_oecd.columns:\n",
    "    df_oecd['time_period'] = pd.to_numeric(df_oecd['time_period'], errors='coerce')\n",
    "\n",
    "# Convert obs_value to numeric and fill missing with 0\n",
    "if 'obs_value' in df_oecd.columns:\n",
    "    df_oecd['obs_value'] = pd.to_numeric(df_oecd['obs_value'], errors='coerce').fillna(0)\n",
    "\n",
    "# Drop duplicate rows\n",
    "df_oecd.drop_duplicates(inplace=True)\n",
    "\n",
    "# Drop columns that are entirely empty\n",
    "df_oecd = drop_empty_columns(df_oecd)\n",
    "\n",
    "# Optimize numeric types to reduce memory usage\n",
    "df_oecd = optimize_numeric_types(df_oecd)\n",
    "\n",
    "# Handle categorical columns if any\n",
    "df_oecd = handle_categorical_data(df_oecd)\n",
    "\n",
    "# Log info after cleaning\n",
    "log_dataframe_info(df_oecd, \"OECD (Cleaned)\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Cleaning the US Death Rates by Vaccination Status dataset (df_us)\n",
    "# ----------------------------------------------------\n",
    "df_us = standardize_column_names(df_us)\n",
    "df_us = parse_dates(df_us, ['day'])\n",
    "\n",
    "# If 'code' column is always null, let's remove it\n",
    "if 'code' in df_us.columns and df_us['code'].isnull().all():\n",
    "    df_us.drop(columns=['code'], inplace=True)\n",
    "\n",
    "df_us = fill_numeric_nas(df_us, fill_value=0)\n",
    "df_us.drop_duplicates(inplace=True)\n",
    "\n",
    "# Ensure 'entity' is not missing if it's important (here it represents age groups)\n",
    "df_us = df_us[df_us['entity'].notna()]\n",
    "\n",
    "df_us = drop_empty_columns(df_us)\n",
    "df_us = optimize_numeric_types(df_us)\n",
    "df_us = handle_categorical_data(df_us)\n",
    "df_us = sort_by_date_if_available(df_us, 'day')\n",
    "\n",
    "log_dataframe_info(df_us, \"US Death Rates (Cleaned)\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# OPTIONAL: Removing Outliers (commented out)\n",
    "# If you want to remove outliers, uncomment the lines below:\n",
    "#\n",
    "# df_vdr = remove_outliers(df_vdr)\n",
    "# df_vm = remove_outliers(df_vm)\n",
    "# df_oecd = remove_outliers(df_oecd)\n",
    "# df_us = remove_outliers(df_us)\n",
    "#\n",
    "# Remember to log and re-check data after removing outliers.\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Final Save\n",
    "# Save the cleaned DataFrames to the processed directory.\n",
    "# These cleaned files will be used by the Merge, Analysis, and Visualization notebooks.\n",
    "# ----------------------------------------------------\n",
    "df_vdr.to_csv('../data/processed/covid-vaccinations-vs-covid-death-rate_cleaned.csv', index=False)\n",
    "df_vm.to_csv('../data/processed/covid-vaccine-doses-by-manufacturer_cleaned.csv', index=False)\n",
    "df_oecd.to_csv('../data/processed/OECD_health_expenditure_cleaned.csv', index=False)\n",
    "df_us.to_csv('../data/processed/united-states-rates-of-covid-19-deaths-by-vaccination-status_cleaned.csv', index=False)\n",
    "\n",
    "logging.info(\"=== Data cleaning complete! All cleaned files saved to ../data/processed/ ===\")\n",
    "\n",
    "print(\"Data cleaning complete! Please check 'cleaning.log' for a detailed record of the steps taken.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning complete! Please check 'cleaning.log' for a detailed record of the steps taken.\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
